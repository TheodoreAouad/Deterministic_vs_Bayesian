{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_tuto.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "pycharm-791ac80d",
      "language": "python",
      "display_name": "PyCharm (BayesianFewShotExperiments)"
    },
    "accelerator": "GPU",
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxXSsaJooLtX",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": "## Imports"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmy6VJEGmgfj",
        "colab_type": "code",
        "outputId": "ec604b5c-9858-4d3d-bfbd-a38eda142c17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "pycharm": {}
      },
      "source": "",
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "cpu\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\n\nif torch.cuda.is_available():\n    device \u003d \"cuda\"\nelse:\n    device \u003d \"cpu\"\ndevice \u003d torch.device(device)\nprint(device)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% Imports\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "turssu_AoHo0",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": "## Get data"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_4-ekhCndsq",
        "colab_type": "code",
        "colab": {},
        "pycharm": {
          "is_executing": false
        }
      },
      "source": "# %% Datasets\n\ntransform \u003d transforms.ToTensor()\n\ntrainset \u003d torchvision.datasets.MNIST(root\u003d\u0027./data\u0027,train\u003dTrue, transform\u003dtransform ,download\u003dTrue)\ntrainloader \u003d torch.utils.data.DataLoader(trainset, batch_size\u003d16, shuffle\u003dTrue)\n\ntestset \u003d torchvision.datasets.MNIST(root\u003d\u0027./data\u0027,train\u003dFalse, transform\u003dtransform ,download\u003dTrue)\ntestloader \u003d torch.utils.data.DataLoader(testset, batch_size\u003d16, shuffle\u003dTrue)",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]",
            "\r  0%|          | 0/9912422 [00:00\u003c?, ?it/s]",
            "\r  0%|          | 24576/9912422 [00:00\u003c00:44, 221357.74it/s]",
            "\r  1%|          | 98304/9912422 [00:00\u003c00:36, 268547.77it/s]",
            "\r  2%|▏         | 172032/9912422 [00:00\u003c00:29, 330323.47it/s]",
            "\r  3%|▎         | 270336/9912422 [00:00\u003c00:24, 395583.04it/s]",
            "\r  3%|▎         | 344064/9912422 [00:00\u003c00:21, 454993.04it/s]",
            "\r  5%|▍         | 450560/9912422 [00:00\u003c00:17, 548724.36it/s]",
            "\r  6%|▌         | 548864/9912422 [00:01\u003c00:15, 617390.85it/s]",
            "\r  6%|▋         | 630784/9912422 [00:01\u003c00:14, 660837.44it/s]",
            "\r  8%|▊         | 745472/9912422 [00:01\u003c00:12, 754527.25it/s]",
            "\r  9%|▉         | 868352/9912422 [00:01\u003c00:10, 843109.05it/s]",
            "\r 10%|▉         | 966656/9912422 [00:01\u003c00:10, 870387.87it/s]",
            "\r 11%|█         | 1064960/9912422 [00:01\u003c00:10, 879318.42it/s]",
            "\r 12%|█▏        | 1196032/9912422 [00:01\u003c00:08, 975382.11it/s]",
            "\r 14%|█▎        | 1343488/9912422 [00:01\u003c00:07, 1078517.09it/s]",
            "\r 15%|█▌        | 1490944/9912422 [00:01\u003c00:07, 1172237.71it/s]",
            "\r 16%|█▋        | 1622016/9912422 [00:01\u003c00:07, 1153210.98it/s]",
            "\r 18%|█▊        | 1794048/9912422 [00:02\u003c00:06, 1279633.59it/s]",
            "\r 20%|█▉        | 1966080/9912422 [00:02\u003c00:05, 1356699.44it/s]",
            "\r 22%|██▏       | 2154496/9912422 [00:02\u003c00:05, 1475023.26it/s]",
            "\r 24%|██▎       | 2351104/9912422 [00:02\u003c00:04, 1567807.83it/s]",
            "\r 26%|██▌       | 2539520/9912422 [00:02\u003c00:04, 1643204.92it/s]",
            "\r 28%|██▊       | 2744320/9912422 [00:02\u003c00:04, 1743395.35it/s]",
            "\r 30%|██▉       | 2965504/9912422 [00:02\u003c00:03, 1843664.81it/s]",
            "\r 32%|███▏      | 3203072/9912422 [00:02\u003c00:03, 1961205.77it/s]",
            "\r 35%|███▍      | 3448832/9912422 [00:02\u003c00:03, 2079046.56it/s]",
            "\r 37%|███▋      | 3686400/9912422 [00:02\u003c00:02, 2153331.45it/s]",
            "\r 40%|███▉      | 3923968/9912422 [00:03\u003c00:02, 2212821.29it/s]",
            "\r 42%|████▏     | 4161536/9912422 [00:03\u003c00:02, 2250910.67it/s]",
            "\r 44%|████▍     | 4390912/9912422 [00:03\u003c00:02, 2258122.03it/s]",
            "\r 47%|████▋     | 4644864/9912422 [00:03\u003c00:02, 2321106.76it/s]",
            "\r 49%|████▉     | 4890624/9912422 [00:03\u003c00:02, 2343694.99it/s]",
            "\r 52%|█████▏    | 5136384/9912422 [00:03\u003c00:02, 2343522.31it/s]",
            "\r 54%|█████▍    | 5382144/9912422 [00:03\u003c00:01, 2363351.34it/s]",
            "\r 57%|█████▋    | 5627904/9912422 [00:03\u003c00:01, 2361436.28it/s]",
            "\r 59%|█████▉    | 5873664/9912422 [00:03\u003c00:01, 2371480.48it/s]",
            "\r 62%|██████▏   | 6111232/9912422 [00:04\u003c00:01, 2372149.74it/s]",
            "\r 64%|██████▍   | 6348800/9912422 [00:04\u003c00:01, 2369984.20it/s]",
            "\r 67%|██████▋   | 6594560/9912422 [00:04\u003c00:01, 2388636.91it/s]",
            "\r 69%|██████▉   | 6840320/9912422 [00:04\u003c00:01, 1814674.16it/s]",
            "\r 74%|███████▍  | 7340032/9912422 [00:04\u003c00:01, 2233953.63it/s]",
            "\r 77%|███████▋  | 7634944/9912422 [00:04\u003c00:01, 2268339.37it/s]",
            "\r 80%|███████▉  | 7913472/9912422 [00:04\u003c00:00, 2314594.33it/s]",
            "\r 83%|████████▎ | 8183808/9912422 [00:04\u003c00:00, 2334206.64it/s]",
            "\r 85%|████████▌ | 8445952/9912422 [00:04\u003c00:00, 2347156.63it/s]",
            "\r 88%|████████▊ | 8699904/9912422 [00:05\u003c00:00, 2363207.62it/s]",
            "\r 90%|█████████ | 8953856/9912422 [00:05\u003c00:00, 2315024.40it/s]",
            "\r 93%|█████████▎| 9199616/9912422 [00:05\u003c00:00, 2338955.80it/s]",
            "\r 95%|█████████▌| 9445376/9912422 [00:05\u003c00:00, 2345462.21it/s]",
            "\r 98%|█████████▊| 9691136/9912422 [00:05\u003c00:00, 2353892.50it/s]",
            "\r9920512it [00:05, 1764944.57it/s]                             ",
            "\n",
            "\r0it [00:00, ?it/s]",
            "\r  0%|          | 0/28881 [00:00\u003c?, ?it/s]",
            "\r32768it [00:00, 124088.85it/s]           ",
            "\n\r0it [00:00, ?it/s]",
            "\r  0%|          | 0/1648877 [00:00\u003c?, ?it/s]",
            "\r  1%|▏         | 24576/1648877 [00:00\u003c00:07, 216292.60it/s]",
            "\r  6%|▌         | 98304/1648877 [00:00\u003c00:05, 263127.74it/s]",
            "\r 10%|▉         | 163840/1648877 [00:00\u003c00:04, 315981.03it/s]",
            "\r 15%|█▌        | 253952/1648877 [00:00\u003c00:03, 388358.59it/s]",
            "\r 20%|██        | 335872/1648877 [00:00\u003c00:02, 451108.01it/s]",
            "\r 25%|██▌       | 417792/1648877 [00:00\u003c00:02, 509777.19it/s]",
            "\r 30%|███       | 499712/1648877 [00:00\u003c00:02, 572907.44it/s]",
            "\r 37%|███▋      | 606208/1648877 [00:01\u003c00:01, 662689.06it/s]",
            "\r 43%|████▎     | 704512/1648877 [00:01\u003c00:01, 721817.13it/s]",
            "\r 49%|████▉     | 811008/1648877 [00:01\u003c00:01, 782686.26it/s]",
            "\r 56%|█████▌    | 917504/1648877 [00:01\u003c00:00, 849625.99it/s]",
            "\r 63%|██████▎   | 1032192/1648877 [00:01\u003c00:00, 920861.26it/s]",
            "\r 69%|██████▉   | 1138688/1648877 [00:01\u003c00:00, 953043.12it/s]",
            "\r 77%|███████▋  | 1269760/1648877 [00:01\u003c00:00, 1009780.00it/s]",
            "\r 85%|████████▌ | 1409024/1648877 [00:01\u003c00:00, 1062015.22it/s]",
            "\r 94%|█████████▍| 1556480/1648877 [00:01\u003c00:00, 1119911.80it/s]",
            "\r1654784it [00:01, 827805.58it/s]                              ",
            "\n",
            "\r0it [00:00, ?it/s]",
            "\r  0%|          | 0/4542 [00:00\u003c?, ?it/s]",
            "\r8192it [00:00, 46942.35it/s]            ",
            "\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n",
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\nProcessing...\nDone!\n"
          ],
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzwIgFiJoRfK",
        "colab_type": "code",
        "outputId": "f546d62f-fccc-4066-fa6f-ac6da480d656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "pycharm": {}
      },
      "source": [
        "# %% Plot image\n",
        "\n",
        "iter_train \u003d iter(trainloader)\n",
        "image, label \u003d next(iter_train)\n",
        "plt.imshow(image[0,0,:,:])\n",
        "print(label)\n",
        "plt.show()\n",
        "image \u003dimage.to(device)\n",
        "label \u003d label.to(device)"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([7, 9, 3, 6, 0, 7, 9, 1, 4, 7, 8, 3, 6, 9, 3, 8])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\u003d\n",
            "text/plain": [
              "\u003cFigure size 432x288 with 1 Axes\u003e"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYxaPbZaQROb",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "# %% Def training\n",
        "\n",
        "\n",
        "def train(model, optimizer, criterion, number_of_epochs):\n",
        "    \n",
        "    model.train()\n",
        "    for epoch in range(number_of_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        number_of_data \u003d len(trainloader)\n",
        "        interval \u003d number_of_data // 10\n",
        "        running_loss \u003d 0.0\n",
        "        number_of_correct_labels \u003d 0\n",
        "        number_of_labels \u003d 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels \u003d [x.to(device) for x in data]\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs \u003d model(inputs)\n",
        "            loss \u003d criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss +\u003d loss.item()\n",
        "            predicted_labels \u003d outputs.argmax(1)\n",
        "            number_of_correct_labels +\u003d torch.sum(predicted_labels - labels \u003d\u003d 0).item()\n",
        "            number_of_labels +\u003d labels.size(0)\n",
        "            if i % interval \u003d\u003d interval-1:  \n",
        "                print(f\u0027Train: [{epoch + 1}, {i + 1}/{number_of_data}] loss: {running_loss / number_of_data}, \u0027\n",
        "                      f\u0027Acc: {round(100*number_of_correct_labels/number_of_labels,2)} %\u0027)\n",
        "                running_loss \u003d 0.0\n",
        "\n",
        "    print(\u0027Finished Training\u0027)\n",
        "\n",
        "# %%\n",
        "\n",
        "def test(model):\n",
        "\n",
        "    running_loss \u003d 0.0\n",
        "    number_of_correct_labels \u003d 0\n",
        "    number_of_labels \u003d 0\n",
        "    for i, data in enumerate(testloader, 0):\n",
        "\n",
        "        inputs, labels \u003d [x.to(device) for x in data]\n",
        "        outputs \u003d model(inputs)\n",
        "        predicted_labels \u003d outputs.argmax(1)\n",
        "        number_of_correct_labels +\u003d torch.sum(predicted_labels - labels \u003d\u003d 0).item()\n",
        "        number_of_labels +\u003d labels.size(0)\n",
        "        if i % 2000 \u003d\u003d 1999:  # print every 2000 mini-batches\n",
        "            print(f\u0027 Test: {i + 1} loss: {running_loss / 2000}, \u0027\n",
        "                  f\u0027Acc: {round(100*number_of_correct_labels / number_of_labels, 2)} %\u0027)\n",
        "            running_loss \u003d 0.0\n",
        "    print(f\u0027Test accuracy: {round(100*number_of_correct_labels / number_of_labels, 2)} %\u0027)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLH0HdY1oam8",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdUaBkR6mm4i",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "# %% Define modules\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yIry0bCaBaI",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "def init_weights(channel_out,channel_input,width,height):\n",
        "    k \u003d 1./ (channel_input * width * height)\n",
        "    return (torch.rand(channel_out,channel_input,width,height)-0.5) * torch.sqrt(torch.Tensor([k]))\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98dLH2m0Fpg_",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "class DeterministClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, number_of_classes):\n",
        "        super(DeterministClassifier,self).__init__()\n",
        "        self.conv1 \u003d nn.Conv2d(1,16,3,padding\u003d1)\n",
        "        self.pool1 \u003d nn.MaxPool2d(2,2)\n",
        "        self.conv2 \u003d nn.Conv2d(16,32,3,padding\u003d1)\n",
        "        self.pool2 \u003d nn.MaxPool2d(2,2)\n",
        "\n",
        "        self.fc1 \u003d nn.Linear(32*7*7, number_of_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x \u003d self.pool1(F.relu(self.conv1(x)))\n",
        "        x \u003d self.pool2(F.relu(self.conv2(x)))\n",
        "        x \u003d x.view(-1,32*7*7)\n",
        "        output \u003d F.softmax(self.fc1(x))\n",
        "\n",
        "        return output\n",
        "\n",
        "class ProbabilistClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, number_of_classes):\n",
        "        super(ProbabilistClassifier,self).__init__()\n",
        "        \n",
        "        self.mu1 \u003d nn.Parameter(data\u003dinit_weights(16,1,3,3),requires_grad\u003dTrue)\n",
        "        self.pool1 \u003d nn.MaxPool2d(2,2)\n",
        "        self.mu2 \u003d nn.Parameter(data\u003dinit_weights(32,16,3,3),requires_grad\u003dTrue)\n",
        "        self.pool2 \u003d nn.MaxPool2d(2,2)\n",
        "        \n",
        "        #self.mufc \u003d nn.Parameter(data\u003dtorch.rand(10,16*14*14),requires_grad\u003dTrue)\n",
        "        self.fc1 \u003d nn.Linear(32*7*7, number_of_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weights \u003d \n",
        "        x \u003d self.pool1(F.relu(F.conv2d(x,self.mu1 + self.sigma*random(),padding\u003d1)))\n",
        "        x \u003d self.pool2(F.relu(F.conv2d(x,self.mu2,padding\u003d1)))\n",
        "        x \u003d x.view(-1,32*7*7)\n",
        "        output \u003d F.softmax(self.fc1(x))\n",
        "        #output \u003d F.softmax(F.linear(x, self.mufc))\n",
        "        return output\n",
        "    \n",
        "class ProbabilistClassifier2(nn.Module):\n",
        "    \n",
        "    def __init__(self, number_of_classes):\n",
        "        super(ProbabilistClassifier2,self).__init__()\n",
        "        self.mu1 \u003d nn.Parameter(data\u003dtorch.rand(16,1,3,3),requires_grad\u003dTrue)\n",
        "        self.pool1 \u003d nn.MaxPool2d(2,2)\n",
        "        self.mu2 \u003d nn.Parameter(data\u003dtorch.rand(32,16,3,3),requires_grad\u003dTrue)\n",
        "        self.pool2 \u003d nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        self.mufc \u003d nn.Parameter(data\u003dtorch.rand(10,32*7*7),requires_grad\u003dTrue)\n",
        "       # self.fc1 \u003d nn.Linear(32*7*7, number_of_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 \u003d nn.Conv2d(1,16,3,padding\u003d1).to(device)\n",
        "        conv1.weight.data \u003d self.mu1\n",
        "        conv2 \u003d nn.Conv2d(16,32,3,padding\u003d1).to(device)\n",
        "        conv2.weight.data \u003d self.mu2\n",
        "        \n",
        "        x \u003d self.pool1(F.relu(conv1(x)))\n",
        "        x \u003d self.pool2(F.relu(conv2(x)))\n",
        "        x \u003d x.view(-1,32*7*7)\n",
        "        #output \u003d F.softmax(self.fc1(x))\n",
        "        output \u003d F.softmax(F.linear(x, self.mufc))\n",
        "        return output\n",
        "    \n",
        "class DenseProbabilistClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, number_of_classes):\n",
        "        super(DenseProbabilistClassifier,self).__init__()\n",
        "        self.conv1 \u003d nn.Conv2d(1,16,3,padding\u003d1)\n",
        "        self.pool1 \u003d nn.MaxPool2d(2,2)\n",
        "        self.conv2 \u003d nn.Conv2d(16,32,3,padding\u003d1)\n",
        "        self.pool2 \u003d nn.MaxPool2d(2,2)\n",
        "\n",
        "        #self.fc1 \u003d nn.Linear(32*7*7, number_of_classes)\n",
        "        self.mu1 \u003d nn.Parameter(data\u003dtorch.rand(10,32*7*7),requires_grad\u003dTrue)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x \u003d self.pool1(F.relu(self.conv1(x)))\n",
        "        x \u003d self.pool2(F.relu(self.conv2(x)))\n",
        "        x \u003d x.view(-1,32*7*7)\n",
        "        #output \u003d F.softmax(self.fc1(x))\n",
        "        weights \u003d self.mu1\n",
        "        output \u003d F.softmax(F.linear(x,weights))\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5mnqRhtUW_T",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "model_proba \u003d ProbabilistClassifier(10)\n",
        "model_proba.to(device)\n",
        "adam_proba \u003d optim.Adam(model_proba.parameters())\n",
        "criterion \u003d nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hie0bl8kYQTJ",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "model_proba.mu1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKiM1y2oqA8N",
        "colab_type": "code",
        "outputId": "4476f28b-1970-425c-d0b9-b225a4d718b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "pycharm": {}
      },
      "source": [
        "train(model_proba,adam_proba,criterion,1)\n"
      ],
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim\u003dX as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train: [1, 375/3750] loss: 0.175157230981191, Acc: 72.15 %\n",
            "\u003cNllLossBackward object at 0x7f2550c65cf8\u003e\n",
            "Train: [1, 750/3750] loss: 0.16267022879918416, Acc: 77.97 %\n",
            "\u003cNllLossBackward object at 0x7f2550c651d0\u003e\n",
            "Train: [1, 1125/3750] loss: 0.15572323481241862, Acc: 82.24 %\n",
            "\u003cNllLossBackward object at 0x7f2550c658d0\u003e\n",
            "Train: [1, 1500/3750] loss: 0.15196398865381877, Acc: 85.29 %\n",
            "\u003cNllLossBackward object at 0x7f2550c65748\u003e\n",
            "Train: [1, 1875/3750] loss: 0.15152535219192506, Acc: 87.2 %\n",
            "\u003cNllLossBackward object at 0x7f2550c65748\u003e\n",
            "Train: [1, 2250/3750] loss: 0.1513173193613688, Acc: 88.51 %\n",
            "\u003cNllLossBackward object at 0x7f2550c657b8\u003e\n",
            "Train: [1, 2625/3750] loss: 0.15105015672047933, Acc: 89.47 %\n",
            "\u003cNllLossBackward object at 0x7f2550c657b8\u003e\n",
            "Train: [1, 3000/3750] loss: 0.1503132563908895, Acc: 90.27 %\n",
            "\u003cNllLossBackward object at 0x7f2550c65828\u003e\n",
            "Train: [1, 3375/3750] loss: 0.149904935614268, Acc: 90.95 %\n",
            "\u003cNllLossBackward object at 0x7f2550c651d0\u003e\n",
            "Train: [1, 3750/3750] loss: 0.1500174873352051, Acc: 91.47 %\n",
            "\u003cNllLossBackward object at 0x7f2550c65828\u003e\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds5zqudHQc0E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b42b75ff-060f-4f30-9ec4-c17960bef9fa",
        "pycharm": {}
      },
      "source": [
        "model_proba.zero_grad()\n",
        "output \u003d model_proba(image)\n",
        "loss \u003d criterion(output,label)\n",
        "loss.backward()\n",
        "[(name, torch.abs(k.grad).sum()) if k.grad is not None else (name,k.grad) for (name, k) in model_proba.named_parameters()]"
      ],
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim\u003dX as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\u0027mu1\u0027, tensor(0.5091, device\u003d\u0027cuda:0\u0027)),\n",
              " (\u0027mu2\u0027, tensor(2.7432, device\u003d\u0027cuda:0\u0027)),\n",
              " (\u0027fc1.weight\u0027, tensor(180.2423, device\u003d\u0027cuda:0\u0027)),\n",
              " (\u0027fc1.bias\u0027, tensor(0.0334, device\u003d\u0027cuda:0\u0027))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppvHuLmOXwVL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1343
        },
        "outputId": "7fd4c3f3-bdec-4461-819b-07db3ceb0f0f",
        "pycharm": {}
      },
      "source": [
        "model \u003d DeterministClassifier(10)\n",
        "model.conv1.weight.data"
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-0.3315,  0.3295, -0.0089],\n",
              "          [ 0.1450, -0.3100, -0.1409],\n",
              "          [ 0.0284, -0.3216,  0.0384]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0680,  0.1758,  0.2656],\n",
              "          [ 0.0437,  0.0955, -0.1784],\n",
              "          [-0.2123, -0.3266,  0.0280]]],\n",
              "\n",
              "\n",
              "        [[[-0.2569, -0.2571, -0.2754],\n",
              "          [-0.2883, -0.0703,  0.1990],\n",
              "          [ 0.2720,  0.0206,  0.0062]]],\n",
              "\n",
              "\n",
              "        [[[-0.1546, -0.2378,  0.1914],\n",
              "          [ 0.2666,  0.3301, -0.0437],\n",
              "          [ 0.2658,  0.3119, -0.1413]]],\n",
              "\n",
              "\n",
              "        [[[-0.0411, -0.2840,  0.2892],\n",
              "          [ 0.1991,  0.1910,  0.1826],\n",
              "          [ 0.1032,  0.3029,  0.0534]]],\n",
              "\n",
              "\n",
              "        [[[-0.0357,  0.0159,  0.0182],\n",
              "          [ 0.2724, -0.0514, -0.1378],\n",
              "          [-0.2161,  0.0393, -0.0480]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0095,  0.1137,  0.2032],\n",
              "          [ 0.1709, -0.2291,  0.0639],\n",
              "          [-0.3150,  0.0808,  0.2071]]],\n",
              "\n",
              "\n",
              "        [[[-0.2287,  0.2490, -0.0311],\n",
              "          [ 0.1692, -0.0311,  0.2424],\n",
              "          [-0.0575, -0.2360, -0.1771]]],\n",
              "\n",
              "\n",
              "        [[[-0.1679, -0.0989,  0.0123],\n",
              "          [ 0.0902,  0.3133, -0.1787],\n",
              "          [ 0.1880,  0.0241, -0.0526]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0033,  0.0924,  0.1317],\n",
              "          [ 0.0687, -0.2991, -0.1658],\n",
              "          [-0.2477, -0.2724, -0.0099]]],\n",
              "\n",
              "\n",
              "        [[[-0.1937, -0.1853,  0.1279],\n",
              "          [-0.2890, -0.0374, -0.2912],\n",
              "          [-0.1863, -0.0893, -0.2236]]],\n",
              "\n",
              "\n",
              "        [[[-0.1076, -0.2833, -0.1050],\n",
              "          [-0.2959, -0.0076,  0.2848],\n",
              "          [-0.2954, -0.2089,  0.0691]]],\n",
              "\n",
              "\n",
              "        [[[ 0.2948, -0.0183,  0.2301],\n",
              "          [-0.1629, -0.3240, -0.2968],\n",
              "          [ 0.1565,  0.1250, -0.2495]]],\n",
              "\n",
              "\n",
              "        [[[-0.0045, -0.1142,  0.0236],\n",
              "          [ 0.0363,  0.2186, -0.2336],\n",
              "          [-0.1688, -0.1556, -0.2910]]],\n",
              "\n",
              "\n",
              "        [[[-0.2290,  0.3281,  0.2843],\n",
              "          [ 0.0756,  0.1247, -0.1027],\n",
              "          [ 0.1718, -0.0423,  0.2362]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0990,  0.2683, -0.1021],\n",
              "          [-0.3055, -0.2064,  0.0261],\n",
              "          [-0.2843,  0.0844, -0.2030]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upF8WIrmQ-Qz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "970f9b09-c5bc-46e2-c855-48084d4c4bab",
        "pycharm": {}
      },
      "source": [
        "[(name, torch.abs(k.grad).sum()) if k.grad is not None else (name,k.grad) for (name, k) in model_proba.named_parameters()]"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\u0027mu1\u0027, tensor(0., device\u003d\u0027cuda:0\u0027)),\n",
              " (\u0027mu2\u0027, tensor(0., device\u003d\u0027cuda:0\u0027)),\n",
              " (\u0027fc1.weight\u0027, tensor(0., device\u003d\u0027cuda:0\u0027)),\n",
              " (\u0027fc1.bias\u0027, tensor(0., device\u003d\u0027cuda:0\u0027))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJimGXOIB5y-",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "[(name, k.grad) if k is not None else (name,k) for (name, k) in model_dense_proba.named_parameters()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujUCWICBBfJw",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "model_dense_proba \u003d DenseProbabilistClassifier(nn.Module)\n",
        "model_dense_proba.to(device)\n",
        "adam_proba_dense \u003d optim.Adam(model_dense_proba.parameters())\n",
        "criterion \u003d nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMKNF5jzBuY2",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "train(model_dense_proba,adam_proba_dense,criterion,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwkctLJXs0pw",
        "colab_type": "code",
        "outputId": "3f10055b-a53b-477f-db7e-1e74427fa03e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "pycharm": {}
      },
      "source": [
        "[torch.abs(k.grad).sum() for k in list(model.fc1.parameters())]"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(7.8311, device\u003d\u0027cuda:0\u0027), tensor(0.0102, device\u003d\u0027cuda:0\u0027)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBr3mXloHF2a",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "# %% Definition of model\n",
        "\n",
        "model \u003d DeterministClassifier(number_of_classes\u003d10)\n",
        "model.to(device)\n",
        "sgd \u003d optim.SGD(model.parameters(), lr\u003d0.001, momentum\u003d0.9)\n",
        "adam \u003d optim.Adam(model.parameters())\n",
        "criterion \u003d nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8a1psQ2HGpR",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": "# %% Training\nnumber_of_epochs \u003d 3\ntrain(model, adam, criterion, number_of_epochs)\n\n# %% Testing\nmu1 \u003d nn.Parameter(data\u003dtorch.Tensor(16, 1, (3, 3)), requires_grad\u003dTrue)\nbias1 \u003d nn.Parameter(data\u003dtorch.Tensor(16), requires_grad\u003dTrue)\noutput1 \u003d F.conv2d(image, weight\u003dmu1, bias\u003dbias1, padding\u003d1)\n",
      "execution_count": 0,
      "outputs": []
    }
  ]
}